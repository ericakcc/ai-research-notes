# NYUAD Research Engineer è¡åˆºé€²åº¦

## ç›®æ¨™

2026 å¹´ç”³è«‹ NYUAD CAIR Research Engineerï¼ˆè·¯ç·šåœ–èˆ‡æ‡‰å¾µç­–ç•¥è¦‹ [`topics/NYUAD.md`](topics/NYUAD.md)ï¼‰

## ç¸½è¦½

| éšæ®µ | Topic | é€²åº¦ | ç”¢å‡º |
|------|-------|------|------|
| åŸºç¤ | Vision Transformers | ğŸŸ¡ æ¶æ§‹å°±ç·’ | ViT from scratch, MAE |
| ç¬¬ä¸€éšæ®µ | Self-Supervised Learning | ğŸ”´ æœªé–‹å§‹ | VICReg, SimCLR, Barlow Twins |
| ç¬¬äºŒéšæ®µ | JEPA | ğŸ”´ æœªé–‹å§‹ | I-JEPA, V-JEPA, MC-JEPA |
| ç¬¬ä¸‰éšæ®µ | Reinforcement Learning | ğŸ”´ æœªé–‹å§‹ | PPO, SAC, JEPA+RL |
| ç¬¬ä¸‰éšæ®µ | Embodied AI | ğŸ”´ æœªé–‹å§‹ | Isaac Gym, JEPA Navigator |
| æŒçºŒ | World Models (VMC) | ğŸŸ¡ é€²è¡Œä¸­ | VAE âœ… MDN-RNN ğŸŸ¡ Controller ğŸŸ¡ |

## è©³ç´°é€²åº¦

### Vision Transformers

**01_vit_from_scratch æ¶æ§‹ç‹€æ…‹ï¼š**
- [x] `config.py` â€” ViTConfig dataclassï¼ˆViT-Tiny: dim=256, depth=6, heads=8, patch_size=4ï¼‰
- [x] `model.py` â€” PatchEmbedding, FeedForward, TransformerBlock, VisionTransformer
- [ ] `model.py` â€” **MultiHeadSelfAttention.forward()** â† TODO(human)ï¼Œéœ€è¦ä½ è¦ªæ‰‹å¯¦ä½œ
- [x] `dataset.py` â€” CIFAR-10 DataLoader + augmentation
- [x] `train.py` â€” AdamW + CosineAnnealingLR è¨“ç·´è…³æœ¬
- [x] `test_vit.py` â€” 5 å€‹æ¸¬è©¦ï¼ˆshapeã€residualã€param count ç­‰ï¼‰
- [ ] è·‘é€šæ¸¬è©¦ â€” éœ€å…ˆå®Œæˆ MHSA.forward()
- [ ] è¨“ç·´ CIFAR-10 â€” ç”¢å‡ºï¼šaccuracy > 85%

**ä½ çš„ç¬¬ä¸€å€‹å‹•æ‰‹ä»»å‹™ï¼š**
- æª”æ¡ˆï¼š`topics/vision-transformers/experiments/01_vit_from_scratch/src/model.py`
- æœå°‹ï¼š`TODO(human)`
- ä»»å‹™ï¼šå¯¦ä½œ `MultiHeadSelfAttention.forward()`ï¼ˆç´„ 10 è¡Œï¼‰
- æ ¸å¿ƒï¼šQKV æŠ•å½± â†’ multi-head reshape â†’ scaled dot-product â†’ softmax â†’ åŠ æ¬Šæ±‚å’Œ â†’ è¼¸å‡ºæŠ•å½±
- é©—è­‰ï¼š`uv run pytest topics/vision-transformers/experiments/01_vit_from_scratch/tests/ -v`

**å»ºè­°å…ˆè®€ï¼š**
1. ViT è«–æ–‡ Section 3 (Method) â€” äº†è§£ patch â†’ token çš„æ¦‚å¿µ
2. "Attention Is All You Need" Figure 2 â€” scaled dot-product attention åœ–è§£
3. `lucidrains/vit-pytorch` çš„ Attention class ä½œç‚ºåƒè€ƒ

- [ ] MAE â€” ç”¢å‡ºï¼šmasked autoencoder è¦–è¦ºåŒ–é‡å»ºçµæœ

### Self-Supervised Learning

- [ ] è®€ LeCun EBM Tutorial â€” ç”¢å‡ºï¼šç­†è¨˜ + èƒ½é‡å‡½æ•¸è¦–è¦ºåŒ–
- [ ] VICReg on CIFAR-10 â€” ç”¢å‡ºï¼šfeature decorrelation åˆ†æåœ–
- [ ] SimCLR â€” ç”¢å‡ºï¼šå°æ¯”å­¸ç¿’ t-SNE è¦–è¦ºåŒ–
- [ ] Barlow Twins â€” ç”¢å‡ºï¼šcross-correlation matrix ç†±åŠ›åœ–

### JEPA

- [ ] I-JEPA â€” ç”¢å‡ºï¼šImageNet å­é›†ä¸Šçš„ masked prediction + linear probe
- [ ] V-JEPA â€” ç”¢å‡ºï¼šKinetics å­é›†ä¸Šçš„å½±ç‰‡ç‰¹å¾µé æ¸¬
- [ ] MC-JEPA â€” ç”¢å‡ºï¼šå¤šæ¨¡æ…‹æ¢ä»¶æ³¨å…¥å¯¦é©—

### Reinforcement Learning

- [ ] PPO on CartPole â€” ç”¢å‡ºï¼šreward curve + policy è¦–è¦ºåŒ–
- [ ] SAC on continuous â€” ç”¢å‡ºï¼šLunarLander/HalfCheetah è¨“ç·´çµæœ
- [ ] JEPA features + RL â€” ç”¢å‡ºï¼šç”¨ JEPA è¡¨å¾µä½œç‚ºç‹€æ…‹è¼¸å…¥çš„ RL agent

### Embodied AI

- [ ] Isaac Gym basics â€” ç”¢å‡ºï¼šè·‘é€šå®˜æ–¹ demo + è‡ªå®šç¾©ç’°å¢ƒ
- [ ] JEPA-Driven Navigator â€” ç”¢å‡ºï¼šæœ€çµ‚ Portfolio å°ˆæ¡ˆ

## çŸ¥è­˜åº«å»ºç½®ç´€éŒ„

| æ—¥æœŸ | å®Œæˆé …ç›® |
|------|---------|
| 2026-02-16 | å»ºç«‹ 5 å€‹æ–° topic ç›®éŒ„çµæ§‹ + CLAUDE.md |
| 2026-02-16 | ViT from scratch æ¶æ§‹å®Œæˆï¼ˆå¾… MHSA å¯¦ä½œï¼‰ |
| 2026-02-16 | NYUAD_PROGRESS.md é€²åº¦è¿½è¹¤å»ºç«‹ |

## å­¸ç¿’æ—¥èªŒ

<!-- æ¯æ¬¡å­¸ç¿’å¾Œè¨˜éŒ„ï¼šæ—¥æœŸã€åšäº†ä»€éº¼ã€å­¸åˆ°ä»€éº¼ã€é‡åˆ°çš„å•é¡Œ -->
